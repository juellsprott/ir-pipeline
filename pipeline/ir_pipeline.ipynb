{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Passage (Re)ranking\n",
    "This notebook contains a pipeline that takes the results of a passage full ranker, creates feature vectors with various basic and extended features and then trains models such as Ranknet and XGBOOST using these feature vectors. The results of the fullrankers are stored in the 'output' folder. For this notebook, the index, the passage length index, tokenised queries and tokenised passages are required. Some of these are quite big and as such will be saved to disk after running the notebook once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports and pip installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\juell\\appdata\\roaming\\python\\python39\\site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: numpy in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.21.6)\n",
      "Requirement already satisfied: nltk in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: textblob in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.37.1)\n",
      "Requirement already satisfied: memory_profiler in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: pysos in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: contractions in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.72)\n",
      "Requirement already satisfied: gensim in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: joblib in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\juell\\appdata\\roaming\\python\\python39\\site-packages (from memory_profiler) (5.9.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pysos) (5.0.0)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: anyascii in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\juell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!python3 -V # please make sure this is python 3\n",
    "%pip install torch numpy nltk textblob wheel memory_profiler pysos contractions gensim\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains all module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file management\n",
    "import os\n",
    "import codecs\n",
    "import importlib\n",
    "import pickle\n",
    "import datetime\n",
    "import pysos\n",
    "from pathlib import Path\n",
    "\n",
    "# data structures\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "\n",
    "# math computations\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob as tb\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# computational processes\n",
    "import multiprocessing\n",
    "from process import process_single_passage\n",
    "from functools import partial\n",
    "\n",
    "# indexing and preprocessing\n",
    "import pass_len_index\n",
    "import pysos\n",
    "import small_index\n",
    "import process\n",
    "import passage_preprocess\n",
    "\n",
    "from small_index import SmallIndex\n",
    "from pass_len_index import PassageLengthIndex\n",
    "from passage_preprocess import PassagePP\n",
    "\n",
    "# full rankers\n",
    "import create_vector\n",
    "import full_ranker\n",
    "\n",
    "from full_ranker import Fullranker\n",
    "\n",
    "# rerankers\n",
    "import xgboost_ranker\n",
    "import ranknet\n",
    "import reranker\n",
    "\n",
    "from ranknet import train, inference\n",
    "from xgboost_ranker import transform_dict, transform_dict_experimental, XGBoostRanker\n",
    "from reranker import Reranker\n",
    "\n",
    "# feature vectors\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from create_vector import VectorCreator\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from additional_features import get_awe_similarities, extend_features\n",
    "\n",
    "# evaluation metrics\n",
    "import evaluation_methods as evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Load queries and labels into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_loader(path):\n",
    "    print(\"load passages from: {}\".format(path))   \n",
    "    passages = json.load(open(path, 'r', encoding=\"utf-8\", errors=\"ignore\"))    \n",
    "    return passages\n",
    "\n",
    "def query_loader(path):    \n",
    "    print(\"load queries from: {}\".format(path))\n",
    "    queries = json.load(open(path, 'r'))    \n",
    "    return queries\n",
    "\n",
    "\n",
    "def label_loader(path):\n",
    "    print(\"Load labels from: {}\".format(path))\n",
    "    labels = json.load(open(path, 'r'))    \n",
    "    return labels\n",
    "\n",
    "# NOTE: passages only get loaded in a seperate cell, if required for followup operations\n",
    "queries_training = query_loader(\"../../data/training_queries.json\")\n",
    "queries_validation = query_loader(\"../../data/validation_queries.json\")\n",
    "queries_test = query_loader(\"../../data/test_queries.json\")\n",
    "\n",
    "labels_training = label_loader(\"../../data/training_labels.json\")\n",
    "labels_validation = label_loader(\"../../data/validation_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data class that holds all our data such as queries, labels and passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, queries_training, queries_validation, \n",
    "                 queries_test, labels_training, labels_validation):\n",
    "        # when creating index, passages could be provided\n",
    "        self.passages = None\n",
    "        self.tokenised_passages = None\n",
    "        self.queries_training = queries_training\n",
    "        self.queries_validation = queries_validation\n",
    "        self.queries_test = queries_test\n",
    "        self.labels_training = labels_training\n",
    "        self.labels_validation = labels_validation\n",
    "        \n",
    "\n",
    "data = Data(queries_training, queries_validation,\n",
    "                 queries_test, labels_training, labels_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a Search Engine wrapper class for the full ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, data):\n",
    "        self.full_ranker = None\n",
    "        self.name = \"None\"\n",
    "        self.data = data\n",
    "\n",
    "    def create_file(self, filename):\n",
    "        \"\"\"Generate a filename specific to this named search engine\n",
    "        \"\"\"\n",
    "        assert isinstance(self.name, str)\n",
    "        assert isinstance(filename, str)\n",
    "        return self.name + \"_\" + filename\n",
    "    \n",
    "    def sort_scores(self, scores):\n",
    "        \"\"\"rank the calclulated scores from largest to smallest\"\"\"\n",
    "        for q_id, p2score in tqdm(scores.items()):\n",
    "            sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "            scores[q_id]=sorted_p2score\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    def write_full_rank_results(self, scores, data_name=\"training\"):\n",
    "        \"\"\"Convert a dictionary of queries and the scored passages to a text file\"\"\"\n",
    "\n",
    "        # create result file name\n",
    "        timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d')\n",
    "        result_file = self.create_file(f\"{timestamp}full_ranking_{data_name}_result.text\")\n",
    "\n",
    "        # open text file and write top 100 results into file together with its features\n",
    "        with codecs.open(f\"../../pipeline/output/{result_file}\", \"w\", \"utf-8\") as file:\n",
    "            for q_id, p2score in scores.items():\n",
    "                ranking=0\n",
    "\n",
    "                # loop through top 100 for each query and write the data for each passage\n",
    "                for (p_id, score) in p2score[:100]:\n",
    "                    ranking+=1         \n",
    "                    feature_1 = score\n",
    "                    feature_2 = 0  # len(self.data.passages[p_id]) -> this is for the reranker later           \n",
    "                    file_title = self.create_file(f\"full_ranking_on_the_{data_name}_set\")\n",
    "                    file.write('\\t'.join([q_id, p_id, str(ranking), str(feature_1), str(feature_2), file_title])+os.linesep) \n",
    "\n",
    "        # output the result file\n",
    "        print(\"Produce file {}\".format(f\"output/{result_file}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for preprocessing the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing the queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 6153.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing the queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 5881.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing the queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 5713.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_queries(queries, lemmatizer, stemmer):\n",
    "    \"\"\"Preprocesses all queries\"\"\"\n",
    "    \n",
    "    print(\"Start preprocessing the queries.\")\n",
    "    # initialize the preprocessing results dict\n",
    "    queries_tokenised = {}\n",
    "    \n",
    "    # for every query\n",
    "    for query_id in tqdm(queries.keys()):\n",
    "        # create a text blob of the current query\n",
    "        text_blob = tb(queries[query_id]).words\n",
    "        \n",
    "        # lower words and remove stopwords from text blob\n",
    "        no_stop = [stemmer.stem(word.lower()) for word in text_blob if not word in stopwords]\n",
    "        \n",
    "        # store the preprocessed result\n",
    "        queries_tokenised[query_id] = no_stop\n",
    "    return queries_tokenised\n",
    "\n",
    "\n",
    "def process_all_query_data(data, stemmer=None, lemmatizer=None):\n",
    "    data.tokenised_queries_training = process_queries(data.queries_training, lemmatizer, stemmer)\n",
    "    data.tokenised_queries_validation = process_queries(data.queries_validation, lemmatizer, stemmer)\n",
    "    data.tokenised_queries_test = process_queries(data.queries_test, lemmatizer, stemmer)\n",
    "    return data\n",
    "\n",
    "# We use the PorterStemmer from nltk\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# The queries are preprocessed and stored in our data class\n",
    "data = process_all_query_data(data, stemmer=stemmer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell for preprocessing passages and loading them into memory (also saving to disk if there is none on disk yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tokenised_passages = True\n",
    "\n",
    "if get_tokenised_passages == True:\n",
    "        try:\n",
    "            # try to load tokenised passages from disk\n",
    "            with open(\"tokenised_passages.pickle\", 'rb') as f:\n",
    "                        tokenised_passages = pickle.load(f)\n",
    "        except:\n",
    "            # using a seperate main cell in order to perform multiprocessing during the preprocessing of passages\n",
    "            if __name__ ==  '__main__': \n",
    "                            # load passages into data class\n",
    "                            data.passages = passage_loader(\"../../data/passages_small.json\")\n",
    "                            index_name = \"small\"\n",
    "\n",
    "                            # create passage preprocess object\n",
    "                            indexer = PassagePP(data)\n",
    "            \n",
    "                            # call preprocessing function and load passages into memory\n",
    "                            tokenised_passages = indexer.process_passages(stopwords, stemmer)\n",
    "\n",
    "                            # save passages to disk using pickling\n",
    "                            with open(\"tokenised_passages.pickle\", 'wb') as f:\n",
    "                                pickle.dump(tokenised_passages, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to load the indexes, if they are not on disk create them new and save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load index from disk...\n",
      "Successfully loaded index to memory!\n",
      "Trying to load index from disk...\n",
      "Successfully loaded index to memory!\n"
     ]
    }
   ],
   "source": [
    "# create a document length index for calculating the BM25 score\n",
    "if __name__ ==  '__main__': \n",
    "    try:\n",
    "        # init passage length index object\n",
    "        pass_len = PassageLengthIndex(data)\n",
    "\n",
    "        # attempt to load index from disk\n",
    "        pass_len_index = pass_len.load_pass_len_index(\"small\")\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        # load passages using loader\n",
    "        print(\"Failed to load, because no stored passage length index is found. Will now create a new index.\")\n",
    "        if not data.passages:\n",
    "            data.passages = passage_loader(\"../../data/passages_small.json\")\n",
    "\n",
    "        # init passage length index object\n",
    "        pass_len_index = PassageLengthIndex(data)\n",
    "        pickle_name = 'small'\n",
    "\n",
    "        # create passage length index, save to disk and load into memory\n",
    "        pass_len_index = pass_len_index.create_pass_len_index(stopwords, pickle_name, tokenised_pass=tokenised_passages, stemmer=stemmer,  result=True)\n",
    "\n",
    "\n",
    "# use a seperate main call, for the multiprocessing part of create index\n",
    "if __name__ ==  '__main__': \n",
    "    try:\n",
    "        # init inverted index object\n",
    "        indexer = SmallIndex(data)\n",
    "\n",
    "        # attempt to load inverted index to memory\n",
    "        index = indexer.load_index(\"small\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Failed to load, because no stored index is found. Will now create a new index.\")\n",
    "        if not data.passages:\n",
    "            data.passages = passage_loader(\"../../data/passages_small.json\")\n",
    "\n",
    "        indexer = SmallIndex(data)\n",
    "        index_name = \"small\"\n",
    "        \n",
    "        # create inverted index with tokens from passages as keys and data for each token as value\n",
    "        index = indexer.create_index(index_name, stopwords, stemmer, tokenised_pass=tokenised_passages, result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Ranking\n",
    "Given a user query, full-ranking aims to quickly and roughly rank all passages and return a ranked list of passages.\n",
    "\n",
    "### 4.1\n",
    "a) Create a Fullranker class with scoring methods <br>\n",
    "b) Create search engine instance and assign a fullranker to it <br>\n",
    "c) Assign a name to the search engine\n",
    "\n",
    "### 4.2\n",
    "d) Get full ranker training scores on the tokenised training queries <br>\n",
    "e) Get full ranker validation scores on the tokenised validation queries <br>\n",
    "f) Get full ranker test scores on the tokenised test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search engine properties are made and you can choose which of the 3 datasets you would like to run for scoring, after which you run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a full ranker\n",
    "franker = Fullranker(index)\n",
    "\n",
    "# create the search engine and assign properties\n",
    "search_engine = SearchEngine(data=data)\n",
    "search_engine.full_ranker = franker\n",
    "search_engine.name = \"bm25_temp_results\"\n",
    "\n",
    "SCORE_TRAINING = False\n",
    "SCORE_VALIDATION = True\n",
    "SCORE_TEST = False\n",
    "\n",
    "# parameters for the full rankers, alpha=0 for non smoothed\n",
    "\n",
    "parameters = {\n",
    "'alpha': 0.7,\n",
    "'b':0.6,\n",
    "'k1':0.7,\n",
    "'l':pass_len_index\n",
    "}\n",
    "\n",
    "# save functions in dict for selecting later\n",
    "full_rankers = {'tf':search_engine.full_ranker.tf_score,\n",
    "                'tfidf':search_engine.full_ranker.tf_idf_score,\n",
    "                'bm25':search_engine.full_ranker.BM25_score,\n",
    "                'ql':search_engine.full_ranker.query_likelihood\n",
    "                }\n",
    "\n",
    "chosen_ranker = 'bm25'\n",
    "ranker_function = full_rankers[chosen_ranker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell computes the scores on the datasets chosen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCORE_TRAINING:\n",
    "    # For each query, calculte scores of all passages on the training set.\n",
    "    training_scores = ranker_function(search_engine.data.tokenised_queries_training, **parameters)\n",
    "\n",
    "    # rank the calclulated scores from largest to smallest.\n",
    "    training_scores = search_engine.sort_scores(training_scores)\n",
    "\n",
    "    # write results to text file\n",
    "    search_engine.write_full_rank_results(training_scores, data_name=\"training\")\n",
    "\n",
    "if SCORE_VALIDATION:\n",
    "    # For each query, calculte scores of all passages on the validation set.\n",
    "    val_scores = ranker_function(search_engine.data.tokenised_queries_validation, **parameters)\n",
    "\n",
    "    # rank the calclulated scores from largest to smallest.\n",
    "    val_scores = search_engine.sort_scores(val_scores)\n",
    "\n",
    "    # write results to text file\n",
    "    search_engine.write_full_rank_results(val_scores, data_name=\"validation\")     \n",
    "\n",
    "if SCORE_TEST:\n",
    "    # For each query, calculte scores of all passages on the test set.\n",
    "    test_scores = ranker_function(search_engine.data.tokenised_queries_test, **parameters)\n",
    "\n",
    "    # rank the calclulated scores from largest to smallest.\n",
    "    test_scores = search_engine.sort_scores(test_scores)\n",
    "\n",
    "    # write results to text file\n",
    "    search_engine.write_full_rank_results(test_scores, data_name=\"test\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the basic feature vector with BM25 as main score (though the chosen full ranker results can be changed if desired using the 'path_keywords' list of tokens) and the other models as additional features. Also contains features such as query term count and passage term count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain result path based on keywords\n",
    "def get_results(search_tokens, large=False):\n",
    "    results = []\n",
    "    search_path = '../../pipeline/output/'\n",
    "    \n",
    "    # check all full ranker output folders\n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        for file in files:\n",
    "            results.append(str(os.path.join(root, file)))\n",
    "\n",
    "    # loop through found filenames\n",
    "    for path in results:\n",
    "\n",
    "            # check if all keywords are present in path name\n",
    "            if all(tokens in path for tokens in search_tokens):\n",
    "\n",
    "                # if using results from fullranker on the large passages dataset, return those\n",
    "                if large == True:\n",
    "                    if 'large' in path:\n",
    "                        return path\n",
    "                else:\n",
    "                    if 'large' not in path:\n",
    "                        return path\n",
    "\n",
    "# get feature vector for reranking\n",
    "def get_features(dataset='training', path_keywords=['bm25', 'training']):\n",
    "    path = get_results(path_keywords)\n",
    "    print(path)\n",
    "\n",
    "    # insert labels and queries for each dataset in the dict\n",
    "    params = {\n",
    "        'training':[data.labels_training, data.tokenised_queries_training],\n",
    "        'validation':[data.labels_validation, data.tokenised_queries_validation],\n",
    "        'test':[None, data.tokenised_queries_test]\n",
    "    }\n",
    "\n",
    "    # init VectorCreator object to get feature vectors\n",
    "    create_vec = VectorCreator(index, pass_len_index, params[dataset][0], params[dataset][1], path)\n",
    "\n",
    "    # get the features \n",
    "    features = create_vec.get_vectors()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load or create feature vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load locally saved feature vectors for training\n",
    "if os.path.isfile(\"features_training.pickle\"):\n",
    "    with open(\"features_training.pickle\", 'rb') as f:\n",
    "        features_training = pickle.load(f)\n",
    "else:\n",
    "    features_training = get_features('training', ['bm25', 'training'])\n",
    "    with open(\"features_training.pickle\", 'wb') as f:\n",
    "        pickle.dump(features_training, f)\n",
    "\n",
    "# load locally saved feature vectors for validation\n",
    "if os.path.isfile(\"features_validation.pickle\"):\n",
    "    with open(\"features_validation.pickle\", 'rb') as f:\n",
    "        features_validation = pickle.load(f)\n",
    "else:\n",
    "    features_validation = get_features('validation', ['bm25', 'validation'])\n",
    "    with open(\"features_validation.pickle\", 'wb') as f:\n",
    "        pickle.dump(features_validation, f)\n",
    "\n",
    "# load locally saved feature vectors for test\n",
    "if os.path.isfile(\"features_test.pickle\"):\n",
    "    with open(\"features_test.pickle\", 'rb') as f:\n",
    "        features_test = pickle.load(f)\n",
    "else:\n",
    "    features_test = get_features('test', ['bm25', 'test'])\n",
    "    with open(\"features_test.pickle\", 'wb') as f:\n",
    "        pickle.dump(features_test, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend features with LSA and AWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "awe_model = gensim.downloader.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get awe matrix for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get awe matrix for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get awe matrix for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7983/7983 [02:33<00:00, 52.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# enable or disable additional features\n",
    "USE_LSA_FEATURES = True\n",
    "USE_AWE_FEATURES = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if USE_AWE_FEATURES == True:\n",
    "    print(\"get awe matrix for validation\")\n",
    "    qids = list(features_validation.keys())\n",
    "    pids = [list(features_validation[q_id].keys()) for q_id in qids]\n",
    "    distances_awe_validation = get_awe_similarities(qids, data.tokenised_queries_validation, pids, data.tokenised_passages, awe_model)\n",
    "\n",
    "    print(\"get awe matrix for test\")\n",
    "    qids = list(features_test.keys())\n",
    "    pids = [list(features_test[q_id].keys()) for q_id in qids]\n",
    "    distances_awe_test = get_awe_similarities(qids, data.tokenised_queries_test, pids, data.tokenised_passages, awe_model)\n",
    "\n",
    "    print(\"get awe matrix for training\")\n",
    "    qids = list(features_training.keys())\n",
    "    pids = [list(features_training[q_id].keys()) for q_id in qids]\n",
    "    distances_awe_training = get_awe_similarities(qids, data.tokenised_queries_training, pids, data.tokenised_passages, awe_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LSA_FEATURES = True\n",
    "features_training = extend_features(features_training, data.queries_training, data.passages, awe_scores=distances_awe_training, awe=USE_AWE_FEATURES, lsa=USE_LSA_FEATURES)\n",
    "features_validation = extend_features(features_validation, data.queries_validation, data.passages, awe_scores=distances_awe_validation, awe=USE_AWE_FEATURES, lsa=USE_LSA_FEATURES)\n",
    "features_test = extend_features(features_test, data.queries_test, data.passages, awe_scores=distances_awe_test, awe=USE_AWE_FEATURES, lsa=USE_LSA_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object that holds all the hyperparameters for RankNet and set the seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", type=int, default=30)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--input_size\", type=int, default=7)\n",
    "parser.add_argument(\"--hidden_size1\", type=int, default=256)\n",
    "parser.add_argument(\"--hidden_size2\", type=int, default=512)\n",
    "parser.add_argument(\"--output_size\", type=int, default=1)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "parser.add_argument(\"--random_seed\", type=int, default=0)\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "torch.cuda.manual_seed_all(args.random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(xgboost_ranker)\n",
    "importlib.reload(reranker)\n",
    "importlib.reload(ranknet)\n",
    "\n",
    "# init reranker object\n",
    "rranker = Reranker(features_training, features_validation, 'm4-reranker-tuning', ranknet_args=args, features_test=features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transforming dict to df, fitting...\n",
      "Fit successful, predicting...\n",
      "predicting...\n",
      "Predict successful, saving results...\n",
      "Produce file reranker_output/test/bm25_awe_and_lsa_vector_xgboost_test_result.text\n"
     ]
    }
   ],
   "source": [
    "# run rerankers and get the result file paths for evaluation\n",
    "xgboost_results_path = rranker.xgboost_reranker(test=False, awe=True, lsa=True)\n",
    "ranknet_results_path = rranker.ranknet_reranker(test=False, awe=True, lsa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: XGBoost BM25 AWE+LSA vector, Path: reranker_output/validation/bm25_awe_and_lsa_vector_xgboost_validation_result.text\n",
      "Name: RankNet BM25 AWE+LSA vector, Path: reranker_output/validation/bm25_awe_and_lsa_vector_ranknet_validation_result.text\n",
      "Name: XGBoost BM25 AWE+LSA  vector, Path: reranker_output/validation/bm25_awe_and_lsa_vector_xgboost_validation_result.text\n",
      "Name: RankNet BM25 AWE+LSA  vector, Path: reranker_output/validation/bm25_awe_and_lsa_vector_ranknet_validation_result.text\n",
      "Name: XGBoost BM25 AWE vector, Path: reranker_output/validation/bm25_awe_vector_xgboost_validation_result.text\n",
      "Name: RankNet BM25 AWE vector, Path: reranker_output/validation/bm25_awe_vector_ranknet_validation_result.text\n",
      "Name: XGBoost-BM25 basic, Path: reranker_output/validation/bm25_basic_vector_xgboost_validation_result.text\n",
      "Name: Ranknet-BM25 basic, Path: reranker_output/validation/bm25_basic_vector_ranknet_validation_result.text\n",
      "Name: BM25, Path: ../../milestones/milestone_2/output/small_passage_results/validation_results/M2_tok-low-stop-stem_bm25_full_ranking_validation_result.text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>MRR</th>\n",
       "      <th>DCG</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Precision@20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost BM25 AWE+LSA vector</td>\n",
       "      <td>0.313787</td>\n",
       "      <td>2.272981</td>\n",
       "      <td>0.306167</td>\n",
       "      <td>0.251015</td>\n",
       "      <td>0.108051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RankNet BM25 AWE+LSA vector</td>\n",
       "      <td>0.324395</td>\n",
       "      <td>2.288053</td>\n",
       "      <td>0.315676</td>\n",
       "      <td>0.262394</td>\n",
       "      <td>0.110051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost BM25 AWE+LSA  vector</td>\n",
       "      <td>0.313787</td>\n",
       "      <td>2.272981</td>\n",
       "      <td>0.306167</td>\n",
       "      <td>0.251015</td>\n",
       "      <td>0.108051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RankNet BM25 AWE+LSA  vector</td>\n",
       "      <td>0.324395</td>\n",
       "      <td>2.288053</td>\n",
       "      <td>0.315676</td>\n",
       "      <td>0.262394</td>\n",
       "      <td>0.110051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost BM25 AWE vector</td>\n",
       "      <td>0.301692</td>\n",
       "      <td>2.274620</td>\n",
       "      <td>0.299996</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.109051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RankNet BM25 AWE vector</td>\n",
       "      <td>0.314573</td>\n",
       "      <td>2.290427</td>\n",
       "      <td>0.311641</td>\n",
       "      <td>0.252779</td>\n",
       "      <td>0.110551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost-BM25 basic</td>\n",
       "      <td>0.298728</td>\n",
       "      <td>2.274026</td>\n",
       "      <td>0.298606</td>\n",
       "      <td>0.239628</td>\n",
       "      <td>0.110051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ranknet-BM25 basic</td>\n",
       "      <td>0.318111</td>\n",
       "      <td>2.288484</td>\n",
       "      <td>0.312633</td>\n",
       "      <td>0.256371</td>\n",
       "      <td>0.110301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.316362</td>\n",
       "      <td>2.282817</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>0.257047</td>\n",
       "      <td>0.111051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name       MRR       DCG   NDCG@20       MAP  \\\n",
       "0   XGBoost BM25 AWE+LSA vector  0.313787  2.272981  0.306167  0.251015   \n",
       "1   RankNet BM25 AWE+LSA vector  0.324395  2.288053  0.315676  0.262394   \n",
       "2  XGBoost BM25 AWE+LSA  vector  0.313787  2.272981  0.306167  0.251015   \n",
       "3  RankNet BM25 AWE+LSA  vector  0.324395  2.288053  0.315676  0.262394   \n",
       "4       XGBoost BM25 AWE vector  0.301692  2.274620  0.299996  0.242378   \n",
       "5       RankNet BM25 AWE vector  0.314573  2.290427  0.311641  0.252779   \n",
       "6            XGBoost-BM25 basic  0.298728  2.274026  0.298606  0.239628   \n",
       "7            Ranknet-BM25 basic  0.318111  2.288484  0.312633  0.256371   \n",
       "8                          BM25  0.316362  2.282817  0.313500  0.257047   \n",
       "\n",
       "   Precision@20  \n",
       "0      0.108051  \n",
       "1      0.110051  \n",
       "2      0.108051  \n",
       "3      0.110051  \n",
       "4      0.109051  \n",
       "5      0.110551  \n",
       "6      0.110051  \n",
       "7      0.110301  \n",
       "8      0.111051  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def run_evaluation(df, model_name, path=\"../output/re_ranking_validation_result.text\", debug=False, tuning_results=False):\n",
    "    print(f\"Name: {model_name}, Path: {path}\")\n",
    "    \n",
    "    # create score storage (for every query a list of passages)\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    # open the results from the model path\n",
    "    with codecs.open(path, \"r\", \"utf-8\") as file:\n",
    "        # store every query result in the scores dictionary\n",
    "        for line in file.readlines():\n",
    "            content = line.split('\\t')\n",
    "            scores[content[0]].append(content[1])\n",
    "    \n",
    "    # Flag for debugging\n",
    "    if debug:\n",
    "        print(f\"Number of passages: {len(list(scores.items())[0][1])}\")\n",
    "        print(f\"Number of queries: {len(scores)}\")\n",
    "    \n",
    "    # run MRR evaluation\n",
    "    mrr = evals.MRR(queries=queries_validation, labels=labels_validation)\n",
    "    mrr_res = mrr.evaluate(scores, k=100)\n",
    "    \n",
    "\n",
    "    # run DCG evaluation\n",
    "    dcg = evals.DCG(queries=queries_validation, labels=labels_validation)\n",
    "    dcg_res = dcg.evaluate(scores, k=100)\n",
    "    \n",
    "\n",
    "    # run nDCG evaluation\n",
    "    ndcg = evals.nDCG(queries=queries_validation, labels=labels_validation)\n",
    "    ndcg_res = ndcg.evaluate(scores, k=20)\n",
    "    \n",
    "    \n",
    "    # run nDCG evaluation\n",
    "    precision = evals.Precision(queries=queries_validation, labels=labels_validation)\n",
    "    precision_res = precision.evaluate(scores, k=20)\n",
    "    \n",
    "    \n",
    "    # run MAP evaluation (mean for all queries in total)\n",
    "    map = evals.MAP(queries=queries_validation, labels=labels_validation)\n",
    "    # print(map.labels)\n",
    "    map_res = map.evaluate(scores)\n",
    "    \n",
    "\n",
    "    if debug:\n",
    "        print(\"Full-ranking:\")\n",
    "        print(\"............\")\n",
    "        print('MRR@{}: {:.4f}'.format(100, mrr_res))\n",
    "        print('DCG@{}: {:.4f}'.format(100, dcg_res))\n",
    "        print('nDCG@{}: {:.4f}'.format(20, ndcg_res))\n",
    "        print('Precision@{}: {:.4f}'.format(20, precision_res))\n",
    "        print('MAP: {:.4f}'.format(map_res))\n",
    "        print(\"............\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    if tuning_results is False:\n",
    "    # add model_name, MRR, DCG, NDCG, MAP\n",
    "        df.loc[len(df.index)] = [model_name, mrr_res, dcg_res, ndcg_res, map_res, precision_res]\n",
    "    else:\n",
    "        return model_name, mrr_res, dcg_res, ndcg_res, map_res, precision_res\n",
    "\n",
    "# initialize dataframe\n",
    "df = pd.DataFrame(columns=['model_name', 'MRR', 'DCG', 'NDCG@20', \"MAP\", \"Precision@20\"])\n",
    "\n",
    "\n",
    "\n",
    "run_evaluation(df, model_name=f\"XGBoost BM25 AWE+LSA vector\", path=xgboost_results_path)\n",
    "\n",
    "run_evaluation(df, model_name=f\"RankNet BM25 AWE+LSA vector\", path=ranknet_results_path)\n",
    "\n",
    "\n",
    "# Results from old runs are shown here\n",
    "run_evaluation(df, model_name=f\"XGBoost BM25 AWE+LSA  vector\", path='reranker_output/validation/bm25_awe_and_lsa_vector_xgboost_validation_result.text')\n",
    "\n",
    "run_evaluation(df, model_name=f\"RankNet BM25 AWE+LSA  vector\", path='reranker_output/validation/bm25_awe_and_lsa_vector_ranknet_validation_result.text')\n",
    "\n",
    "run_evaluation(df, model_name=f\"XGBoost BM25 AWE vector\", path='reranker_output/validation/bm25_awe_vector_xgboost_validation_result.text')\n",
    "\n",
    "run_evaluation(df, model_name=f\"RankNet BM25 AWE vector\", path='reranker_output/validation/bm25_awe_vector_ranknet_validation_result.text')\n",
    "\n",
    "run_evaluation(df, model_name=f\"XGBoost-BM25 basic\", path=f\"reranker_output/validation/bm25_basic_vector_xgboost_validation_result.text\")\n",
    "\n",
    "run_evaluation(df, model_name=f\"Ranknet-BM25 basic\", path=f\"reranker_output/validation/bm25_basic_vector_ranknet_validation_result.text\")\n",
    "\n",
    "run_evaluation(df, model_name=f\"BM25\", path=f\"../../milestones/milestone_2/output/small_passage_results/validation_results/M2_tok-low-stop-stem_bm25_full_ranking_validation_result.text\")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb0866d0a9f14e427738d014a94215f474f972fd39e7511024c66f6916292d7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
